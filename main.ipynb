{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1192f571",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import h5py\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from monai.transforms import Compose, ScaleIntensity, EnsureType\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- CONFIG ---\n",
    "BATCH_SIZE = 16 # Diffusion needs smaller batches usually due to VRAM\n",
    "LR = 1e-4\n",
    "MAX_EPOCHS = 50\n",
    "TIMESTEPS = 1000  # Diffusion steps\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "DATA_DIR = r\"C:\\Users\\shara\\.cache\\kagglehub\\datasets\\awsaf49\\brats2020-training-data\\versions\\3\\BraTS2020_training_data\\content\\data\"\n",
    "META_FILE = r\"C:\\Users\\shara\\.cache\\kagglehub\\datasets\\awsaf49\\brats2020-training-data\\versions\\3\\BraTS2020_training_data\\content\\data\\meta_data.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f928e891",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â³ Preparing Dataframes...\n",
      "âœ… Data Ready: Train Size 10658 | Val Size 3100\n"
     ]
    }
   ],
   "source": [
    "print(\"â³ Preparing Dataframes...\")\n",
    "df = pd.read_csv(META_FILE)\n",
    "\n",
    "# 1. Setup Paths\n",
    "df['filename'] = df['slice_path'].apply(lambda x: os.path.basename(x))\n",
    "df['local_path'] = df['filename'].apply(lambda x: os.path.join(DATA_DIR, x))\n",
    "\n",
    "# 2. Extract Slice Number (Assumes filename format like '...slice_123.h5')\n",
    "# Adjust the split logic if your filenames are different!\n",
    "df['slice_num'] = df['filename'].apply(lambda x: int(x.split('_')[-1].split('.')[0]))\n",
    "\n",
    "# 3. Create a Lookup Dictionary: (Volume_ID, Slice_Num) -> File_Path\n",
    "# This allows us to find neighbors even if the dataset is shuffled\n",
    "SLICE_MAP = df.set_index(['volume', 'slice_num'])['local_path'].to_dict()\n",
    "\n",
    "# 4. Splitting & Balancing (Your original logic)\n",
    "unique_patients = df['volume'].unique()[:100]\n",
    "train_ids, val_ids = train_test_split(unique_patients, test_size=0.2, random_state=42)\n",
    "\n",
    "train_df_raw = df[df['volume'].isin(train_ids)]\n",
    "val_df = df[df['volume'].isin(val_ids)]\n",
    "\n",
    "# Balance Tumor/Healthy\n",
    "df_tumor = train_df_raw[train_df_raw['target'] == 1]\n",
    "df_healthy = train_df_raw[train_df_raw['target'] == 0]\n",
    "df_healthy_balanced = df_healthy.sample(n=len(df_tumor), random_state=42)\n",
    "\n",
    "train_df = pd.concat([df_tumor, df_healthy_balanced])\n",
    "train_df = train_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "print(f\"âœ… Data Ready: Train Size {len(train_df)} | Val Size {len(val_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ddd274",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BraTS25DDiffusionDataset(Dataset):\n",
    "    def __init__(self, dataframe, slice_map, transform=None):\n",
    "        self.dataframe = dataframe.reset_index(drop=True)\n",
    "        self.slice_map = slice_map\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def load_file(self, path):\n",
    "        \"\"\"Helper to safely load one H5 file\"\"\"\n",
    "        if path is None: return None # Handle missing neighbors\n",
    "        try:\n",
    "            with h5py.File(path, 'r') as f:\n",
    "                img = f['image'][:] \n",
    "                mask = f['mask'][:]\n",
    "            return img, mask\n",
    "        except:\n",
    "            return None, None\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.dataframe.iloc[idx]\n",
    "        vol_id = row['volume']\n",
    "        center_slice = row['slice_num']\n",
    "        \n",
    "        # 1. Identify Paths for [Previous, Center, Next]\n",
    "        slice_indices = [center_slice - 1, center_slice, center_slice + 1]\n",
    "        stack_imgs = []\n",
    "        \n",
    "        # 2. Load the 2.5D Stack\n",
    "        for s_idx in slice_indices:\n",
    "            path = self.slice_map.get((vol_id, s_idx))\n",
    "            \n",
    "            # If neighbor doesn't exist (edge case), use the center slice again (padding)\n",
    "            if path is None: \n",
    "                path = self.slice_map.get((vol_id, center_slice))\n",
    "            \n",
    "            img, _ = self.load_file(path) # We don't need neighbor masks, only images\n",
    "            \n",
    "            # Fix Shape: (H, W, 4) -> (4, H, W)\n",
    "            if img is not None:\n",
    "                if img.ndim == 3 and img.shape[-1] == 4:\n",
    "                    img = np.transpose(img, (2, 0, 1))\n",
    "                stack_imgs.append(torch.tensor(img, dtype=torch.float32))\n",
    "            else:\n",
    "                # Fallback if file corrupt\n",
    "                return self.__getitem__((idx + 1) % len(self.dataframe))\n",
    "\n",
    "        # Stack shape: [3 (slices), 4 (modalities), H, W] -> Flatten to [12, H, W]\n",
    "        # This is the \"Condition\" (MRI Context)\n",
    "        condition_stack = torch.cat(stack_imgs, dim=0) \n",
    "\n",
    "        # 3. Load Target Mask (Center Slice Only)\n",
    "        _, mask = self.load_file(self.slice_map.get((vol_id, center_slice)))\n",
    "        if mask.ndim == 3: mask = mask[:, :, 0]\n",
    "        wt = (mask > 0).astype(np.float32)\n",
    "        wt = (wt * 2) - 1\n",
    "        target_mask = torch.tensor(wt[None, :, :], dtype=torch.float32) # [1, H, W]\n",
    "\n",
    "        # Apply transforms if needed (apply same transform to stack and mask manually if doing spatial augs)\n",
    "        if self.transform:\n",
    "            condition_stack = self.transform(condition_stack)\n",
    "            # Ensure mask isn't scaled weirdly by transforms intended for images\n",
    "            \n",
    "        return condition_stack, target_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "36cd1945",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiffusionUNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Input = 1 (Noisy Mask) + 12 (3 slices * 4 modalities) = 13 Channels\n",
    "        self.inc = nn.Sequential(\n",
    "            nn.Conv2d(13, 64, 3, padding=1),\n",
    "            nn.GroupNorm(8, 64),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        # x1 output: 64 channels\n",
    "        \n",
    "        self.down1 = nn.Sequential(nn.MaxPool2d(2), nn.Conv2d(64, 128, 3, padding=1), nn.GroupNorm(8, 128), nn.ReLU())\n",
    "        # x2 output: 128 channels\n",
    "        \n",
    "        self.down2 = nn.Sequential(nn.MaxPool2d(2), nn.Conv2d(128, 256, 3, padding=1), nn.GroupNorm(8, 256), nn.ReLU())\n",
    "        # x3 output: 256 channels (passed to bottleneck)\n",
    "        \n",
    "        self.bot = nn.Sequential(nn.Conv2d(256, 512, 3, padding=1), nn.GroupNorm(8, 512), nn.ReLU())\n",
    "\n",
    "        # --- DECODER ---\n",
    "\n",
    "        # UP 1\n",
    "        self.up1 = nn.ConvTranspose2d(512, 256, 2, stride=2) \n",
    "        # up1 output: 256. \n",
    "        # Concatenates with x2 (128). Total = 256 + 128 = 384.\n",
    "        \n",
    "        # FIX 1: Input channels changed from 512 -> 384\n",
    "        self.up1_conv = nn.Sequential(\n",
    "            nn.Conv2d(384, 256, 3, padding=1), \n",
    "            nn.GroupNorm(8, 256), \n",
    "            nn.ReLU()\n",
    "        ) \n",
    "        \n",
    "        # UP 2\n",
    "        self.up2 = nn.ConvTranspose2d(256, 128, 2, stride=2)\n",
    "        # up2 output: 128.\n",
    "        # Concatenates with x1 (64). Total = 128 + 64 = 192.\n",
    "        \n",
    "        # FIX 2: Input channels changed from 256 -> 192\n",
    "        self.up2_conv = nn.Sequential(\n",
    "            nn.Conv2d(192, 128, 3, padding=1), \n",
    "            nn.GroupNorm(8, 128), \n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.out = nn.Conv2d(128, 1, 1)\n",
    "\n",
    "        # Time Embedding\n",
    "        self.time_mlp = nn.Sequential(\n",
    "            nn.Linear(1, 128),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(128, 128)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, t, condition):\n",
    "        # x: Noisy Mask [B, 1, H, W]\n",
    "        # condition: MRI Stack [B, 12, H, W]\n",
    "        \n",
    "        x_input = torch.cat([x, condition], dim=1) # [B, 13, H, W]\n",
    "        \n",
    "        # Time processing\n",
    "        t = t.float().unsqueeze(-1).to(x.device)\n",
    "        t_emb = self.time_mlp(t).unsqueeze(-1).unsqueeze(-1)\n",
    "        \n",
    "        # Encoder\n",
    "        x1 = self.inc(x_input)         # [B, 64, H, W]\n",
    "        x2 = self.down1(x1)            # [B, 128, H/2, W/2]\n",
    "        x3 = self.down2(x2 + t_emb)    # [B, 256, H/4, W/4]  <-- Add time here\n",
    "        \n",
    "        # Bottleneck\n",
    "        b = self.bot(x3)               # [B, 512, H/4, W/4]\n",
    "        \n",
    "        # Decoder\n",
    "        u1 = self.up1(b)               # [B, 256, H/2, W/2]\n",
    "        \n",
    "        # Resize if dimensions don't match exactly (e.g. odd input sizes)\n",
    "        if u1.shape != x2.shape: u1 = F.interpolate(u1, size=x2.shape[2:])\n",
    "            \n",
    "        u1 = torch.cat([u1, x2], dim=1)# [B, 256+128=384, H/2, W/2]\n",
    "        u1 = self.up1_conv(u1)         # [B, 256, H/2, W/2]\n",
    "        \n",
    "        u2 = self.up2(u1)              # [B, 128, H, W]\n",
    "        \n",
    "        if u2.shape != x1.shape: u2 = F.interpolate(u2, size=x1.shape[2:])\n",
    "            \n",
    "        u2 = torch.cat([u2, x1], dim=1)# [B, 128+64=192, H, W]\n",
    "        u2 = self.up2_conv(u2)         # [B, 128, H, W]\n",
    "        \n",
    "        return self.out(u2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75da74d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Starting Diffusion Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0017: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 667/667 [11:07<00:00,  1.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Avg Loss: 0.02264\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0020: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 667/667 [09:22<00:00,  1.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Avg Loss: 0.00577\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0029:  22%|â–ˆâ–ˆâ–       | 144/667 [02:14<07:19,  1.19it/s]"
     ]
    }
   ],
   "source": [
    "# --- HELPERS ---\n",
    "def get_beta_schedule(timesteps):\n",
    "    return torch.linspace(1e-4, 0.02, timesteps).to(DEVICE)\n",
    "\n",
    "betas = get_beta_schedule(TIMESTEPS)\n",
    "alphas = 1. - betas\n",
    "alphas_cumprod = torch.cumprod(alphas, dim=0)\n",
    "sqrt_alphas_cumprod = torch.sqrt(alphas_cumprod)\n",
    "sqrt_one_minus_alphas_cumprod = torch.sqrt(1. - alphas_cumprod)\n",
    "\n",
    "def forward_diffusion(x_0, t):\n",
    "    \"\"\" Adds noise to the mask \"\"\"\n",
    "    noise = torch.randn_like(x_0)\n",
    "    sqrt_alpha_t = sqrt_alphas_cumprod[t][:, None, None, None]\n",
    "    sqrt_one_minus_alpha_t = sqrt_one_minus_alphas_cumprod[t][:, None, None, None]\n",
    "    x_t = sqrt_alpha_t * x_0 + sqrt_one_minus_alpha_t * noise\n",
    "    return x_t, noise\n",
    "\n",
    "# --- EXECUTION ---\n",
    "train_ds = BraTS25DDiffusionDataset(train_df, SLICE_MAP, transform=Compose([ScaleIntensity(), EnsureType()]))\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=0) # Workers=0 for H5 safety on Windows\n",
    "\n",
    "model = DiffusionUNet().to(DEVICE)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "scaler = torch.amp.GradScaler('cuda') # Fixed new syntax for torch > 2.0\n",
    "\n",
    "print(\"ðŸš€ Starting Diffusion Training...\")\n",
    "\n",
    "for epoch in range(MAX_EPOCHS):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{MAX_EPOCHS}\")\n",
    "    \n",
    "    for batch in progress_bar:\n",
    "        # MRI Stack [B, 12, H, W] (Condition)\n",
    "        # Tumor Mask [B, 1, H, W] (Target)\n",
    "        condition, mask = batch['image'] if isinstance(batch, dict) else batch\n",
    "        \n",
    "        condition = condition.to(DEVICE)\n",
    "        mask = mask.to(DEVICE)\n",
    "        \n",
    "        # 1. Sample Random Timesteps\n",
    "        t = torch.randint(0, TIMESTEPS, (mask.shape[0],), device=DEVICE).long()\n",
    "        \n",
    "        # 2. Add Noise to Mask\n",
    "        noisy_mask, noise = forward_diffusion(mask, t)\n",
    "        \n",
    "        # 3. Train\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        with torch.amp.autocast('cuda'):\n",
    "            # Model predicts the NOISE, given the NoisyMask + CleanMRI + Time\n",
    "            noise_pred = model(noisy_mask, t, condition)\n",
    "            loss = F.mse_loss(noise, noise_pred) # MSE Loss for Diffusion\n",
    "            \n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        progress_bar.set_description(f\"Loss: {loss.item():.4f}\")\n",
    "\n",
    "    print(f\"Epoch {epoch+1} Avg Loss: {epoch_loss / len(train_loader):.5f}\")\n",
    "\n",
    "# Save Model\n",
    "torch.save(model.state_dict(), \"brats_diffusion_25d.pth\")\n",
    "print(\"ðŸ’¾ Model Saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd9fab32",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
